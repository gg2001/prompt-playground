{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from ast import literal_eval\n",
    "from embeddings import CHUNK_SIZE\n",
    "from retrieval import query_embeddings\n",
    "\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/20e01e08-12bd-4258-ab45-5cf9244b727f.csv\")\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(literal_eval)\n",
    "texts, embeddings = df[\"text\"].tolist(), df[\"embedding\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_results(query: str, debug=False) -> list[str]:\n",
    "    results = query_embeddings(query, embeddings, 5)\n",
    "    results_text = [texts[i] for i, _ in results]\n",
    "    if debug:\n",
    "        for i, result in enumerate(results_text):\n",
    "            print(f\"Result {i + 1} (Similarity: {results[i][1]}):\")\n",
    "            print(result)\n",
    "            print(\"-\" * 100)\n",
    "    return results_text\n",
    "\n",
    "\n",
    "def ask(query: str, results_text: list[str], debug=False):\n",
    "    context = \"\\n\\n###\\n\\n\".join(results_text)\n",
    "    system_message = \"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\n\"\n",
    "    user_message = f\"Context: {context}\\n\\n---\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    if debug:\n",
    "        print(\"System message:\")\n",
    "        print(system_message)\n",
    "        print(\"User message:\")\n",
    "        print(user_message)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_message,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1 (Similarity: 0.7596963656376869):\n",
      "With more servers available, some OpenAI leaders believe the company can use its existing AI and recent technical breakthroughs such as Q*—a model that can reason about math problems it hasn’t previously been trained to solve—to create the right synthetic (non–human-generated) data for training better models after running out of human-generated data to give them. These models may also be able to figure out the flaws in existing models like GPT-4 and suggest technical improvements—in other words, self-improving AI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 2 (Similarity: 0.7341242006069101):\n",
      "While some consumers and professionals have embraced ChatGPT and other conversational AI as well as AI-generated video, turning these recent breakthroughs into technology that produces significant revenue could take longer than practitioners in the field anticipated. Firms including Amazon and Google have quietly tempered expectations for sales, in part because such AI is costly and requires a lot of work to launch inside large enterprises or to power new features in apps used by millions of people. Altman said at an Intel event last month that AI models get “predictably better” when researchers throw more computing power at them. OpenAI has published research on this topic, which it refers to as the “scaling laws” of conversational AI. OpenAI “throwing ever more compute [power to scale up existing AI] risks leading to a ‘trough of disillusionment’” among customers as they realize the limits of the technology, said Ali Ghodsi, CEO of Databricks, which helps companies use AI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 3 (Similarity: 0.6890116989246817):\n",
      "“We should really focus on making this technology useful for humans and enterprises. That takes time. I believe it’ll be amazing, but [it] doesn’t happen overnight.” The stakes are high for OpenAI to prove that its next major conversational AI, known as a large language model, is significantly better than GPT-4, its most advanced LLM today. OpenAI released GPT-4 a year ago, and Google has released a comparable model in the meantime as it tries to catch up. OpenAI aims to release its next major LLM upgrade by early next year, said one person with knowledge of the process. It could release more incremental improvements to LLMs before then, this person said.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 4 (Similarity: 0.6376051001527323):\n",
      "An OpenAI spokesperson did not have a comment for this article. Altman has said privately that Google, one of OpenAI’s biggest rivals, will have more computing capacity than OpenAI in the near term, and publicly he has complained about not having as many AI server chips as he’d like. That’s one reason he has been pitching the idea of a new server chip company that would develop a chip rivaling Nvidia’s graphics processing unit, which today powers OpenAI’s software. Demand for Nvidia GPU servers has skyrocketed, driving up costs for customers such as Microsoft and OpenAI. Besides controlling costs, Microsoft has other potential reasons to support Altman’s alternative chip. The GPU boom has put Nvidia in the position of kingmaker as it decides which customers can have the most chips, and it has aided small cloud providers that compete with Microsoft. Nvidia has also muscled into reselling cloud servers to its own customers.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 5 (Similarity: 0.6331188688919396):\n",
      "(OpenAI instead wants to use more generic Ethernet cables.) Switching away from InfiniBand could make it easier for OpenAI and Microsoft to lessen their reliance on Nvidia down the line. AI computing is more expensive and complex than traditional computing, which is why companies closely guard the details about their AI data centers, including how GPUs are connected and cooled. For his part, Nvidia CEO Jensen Huang has said companies and countries will need to build $1 trillion worth of new data centers in the next four to five years to handle all of the AI computing that’s coming. Microsoft and OpenAI executives have been discussing the data center project since at least last summer.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Microsoft working on?\"\n",
    "results_text = query_results(query, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Microsoft is working on a data center project with OpenAI. Additionally, Microsoft has other potential reasons to support an alternative chip pitched by OpenAI's Altman to rival Nvidia’s GPU, which could help control costs and reduce their reliance on Nvidia.\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(query, results_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Microsoft is currently focusing on several key areas, including the advancement of artificial intelligence, cloud computing, and enhancing productivity tools. The company is heavily investing in AI technologies, particularly through its Azure cloud platform, which supports various AI applications and services. Additionally, Microsoft is continuously improving its Microsoft 365 suite, incorporating features like intelligent suggestions and collaboration tools to facilitate remote work and enhance user efficiency. Furthermore, Microsoft is exploring innovations in gaming with its Xbox division, emphasizing cloud gaming and subscription services. The company's commitment to sustainability and security also remains a priority, as it seeks to reduce its carbon footprint and enhance data protection for users. Overall, Microsoft is at the forefront of technological advancements aimed at transforming how individuals and organizations operate in a rapidly evolving digital landscape.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Given a question, generate a paragraph of text that answers the question.\n",
    "Question: {query}\n",
    "Answer:\n",
    "         \"\"\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=CHUNK_SIZE,\n",
    ")\n",
    "\n",
    "generated_text = response.choices[0].message.content\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1 (Similarity: 0.6895649117362705):\n",
      "With more servers available, some OpenAI leaders believe the company can use its existing AI and recent technical breakthroughs such as Q*—a model that can reason about math problems it hasn’t previously been trained to solve—to create the right synthetic (non–human-generated) data for training better models after running out of human-generated data to give them. These models may also be able to figure out the flaws in existing models like GPT-4 and suggest technical improvements—in other words, self-improving AI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 2 (Similarity: 0.6836703203605505):\n",
      "“We should really focus on making this technology useful for humans and enterprises. That takes time. I believe it’ll be amazing, but [it] doesn’t happen overnight.” The stakes are high for OpenAI to prove that its next major conversational AI, known as a large language model, is significantly better than GPT-4, its most advanced LLM today. OpenAI released GPT-4 a year ago, and Google has released a comparable model in the meantime as it tries to catch up. OpenAI aims to release its next major LLM upgrade by early next year, said one person with knowledge of the process. It could release more incremental improvements to LLMs before then, this person said.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 3 (Similarity: 0.6076016664247574):\n",
      "While some consumers and professionals have embraced ChatGPT and other conversational AI as well as AI-generated video, turning these recent breakthroughs into technology that produces significant revenue could take longer than practitioners in the field anticipated. Firms including Amazon and Google have quietly tempered expectations for sales, in part because such AI is costly and requires a lot of work to launch inside large enterprises or to power new features in apps used by millions of people. Altman said at an Intel event last month that AI models get “predictably better” when researchers throw more computing power at them. OpenAI has published research on this topic, which it refers to as the “scaling laws” of conversational AI. OpenAI “throwing ever more compute [power to scale up existing AI] risks leading to a ‘trough of disillusionment’” among customers as they realize the limits of the technology, said Ali Ghodsi, CEO of Databricks, which helps companies use AI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 4 (Similarity: 0.5787063112280189):\n",
      "Such a project is “absolutely required” for artificial general intelligence—AI that can accomplish most of the computing tasks humans do, said Chris Sharp, chief technology officer of Digital Realty, a data center operator that hasn’t been involved in Stargate. Though the project’s scale seems unimaginable by today’s standard, he said that by the time such a supercomputer is finished, the numbers won’t seem as eye-popping. A Microsoft data center near Phoenix that isn't related to OpenAI. Image via Microsoft The executives have discussed launching Stargate as soon as 2028 and expanding it through 2030, possibly needing as much as 5 gigawatts of power by the end, the people involved in the discussions said. Phase Five Altman and Microsoft employees have talked about these supercomputers in terms of five phases, with phase 5 being Stargate, named for a science fiction film in which scientists develop a device for traveling between galaxies.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 5 (Similarity: 0.5779419861781906):\n",
      "An OpenAI spokesperson did not have a comment for this article. Altman has said privately that Google, one of OpenAI’s biggest rivals, will have more computing capacity than OpenAI in the near term, and publicly he has complained about not having as many AI server chips as he’d like. That’s one reason he has been pitching the idea of a new server chip company that would develop a chip rivaling Nvidia’s graphics processing unit, which today powers OpenAI’s software. Demand for Nvidia GPU servers has skyrocketed, driving up costs for customers such as Microsoft and OpenAI. Besides controlling costs, Microsoft has other potential reasons to support Altman’s alternative chip. The GPU boom has put Nvidia in the position of kingmaker as it decides which customers can have the most chips, and it has aided small cloud providers that compete with Microsoft. Nvidia has also muscled into reselling cloud servers to its own customers.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generated_results_text = query_results(generated_text, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The context suggests that Microsoft is involved in discussions and plans surrounding the development of supercomputers, specifically through a project called Stargate. This project involves building a large-scale supercomputer to support advanced AI capabilities, with the goal of launching as soon as 2028 and expanding through 2030. The supercomputer could potentially need as much as 5 gigawatts of power by the end of the project.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(query, generated_results_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
