{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from ast import literal_eval\n",
    "from embeddings import CHUNK_SIZE\n",
    "from retrieval import query_embeddings\n",
    "\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/20e01e08-12bd-4258-ab45-5cf9244b727f.csv\")\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(literal_eval)\n",
    "texts, embeddings = df[\"text\"].tolist(), df[\"embedding\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_results(query: str, debug=True) -> list[str]:\n",
    "    results = query_embeddings(query, embeddings, 5)\n",
    "    results_text = [texts[i] for i, _ in results]\n",
    "    if debug:\n",
    "        for i, result in enumerate(results_text):\n",
    "            print(f\"Result {i + 1} (Similarity: {results[i][1]}):\")\n",
    "            print(result)\n",
    "            print(\"-\" * 100)\n",
    "    return results_text\n",
    "\n",
    "\n",
    "def ask(query: str, results_text: list[str], debug=True):\n",
    "    context = \"\\n\\n###\\n\\n\".join(results_text)\n",
    "    system_message = \"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\n\"\n",
    "    user_message = f\"Context: {context}\\n\\n---\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    if debug:\n",
    "        print(\"System message:\")\n",
    "        print(system_message)\n",
    "        print(\"User message:\")\n",
    "        print(user_message)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_message,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1 (Similarity: 0.9342499421203767):\n",
      "While some consumers and professionals have embraced ChatGPT and other conversational AI as well as AI-generated video, turning these recent breakthroughs into technology that produces significant revenue could take longer than practitioners in the field anticipated. Firms including Amazon and Google have quietly tempered expectations for sales, in part because such AI is costly and requires a lot of work to launch inside large enterprises or to power new features in apps used by millions of people. Altman said at an Intel event last month that AI models get “predictably better” when researchers throw more computing power at them. OpenAI has published research on this topic, which it refers to as the “scaling laws” of conversational AI. OpenAI “throwing ever more compute [power to scale up existing AI] risks leading to a ‘trough of disillusionment’” among customers as they realize the limits of the technology, said Ali Ghodsi, CEO of Databricks, which helps companies use AI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 2 (Similarity: 0.9138110174776263):\n",
      "An OpenAI spokesperson did not have a comment for this article. Altman has said privately that Google, one of OpenAI’s biggest rivals, will have more computing capacity than OpenAI in the near term, and publicly he has complained about not having as many AI server chips as he’d like. That’s one reason he has been pitching the idea of a new server chip company that would develop a chip rivaling Nvidia’s graphics processing unit, which today powers OpenAI’s software. Demand for Nvidia GPU servers has skyrocketed, driving up costs for customers such as Microsoft and OpenAI. Besides controlling costs, Microsoft has other potential reasons to support Altman’s alternative chip. The GPU boom has put Nvidia in the position of kingmaker as it decides which customers can have the most chips, and it has aided small cloud providers that compete with Microsoft. Nvidia has also muscled into reselling cloud servers to its own customers.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 3 (Similarity: 0.9103444111915934):\n",
      "With more servers available, some OpenAI leaders believe the company can use its existing AI and recent technical breakthroughs such as Q*—a model that can reason about math problems it hasn’t previously been trained to solve—to create the right synthetic (non–human-generated) data for training better models after running out of human-generated data to give them. These models may also be able to figure out the flaws in existing models like GPT-4 and suggest technical improvements—in other words, self-improving AI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 4 (Similarity: 0.9047200469498211):\n",
      "Much of the cost of the next two phases will involve procuring the AI chips. Two data center practitioners who aren’t involved in the project said it’s common for AI server chips to make up around half of the total initial cost of AI-focused data centers other companies are currently building. All up, the proposed efforts could cost in excess of $115 billion, more than three times what Microsoft spent last year on capital expenditures for servers, buildings and other equipment. Microsoft was on pace to spend around $50 billion this year, assuming it continues the pace of capital expenditures it disclosed in the second half of 2023. Microsoft CFO Amy Hood said in January that such spending will increase “materially” in the coming quarters, driven by investments in “cloud and AI infrastructure.” Frank Shaw, a Microsoft spokesperson, did not comment about the supercomputing plans but said in a statement: “We are always planning for the next generation of infrastructure innovations needed to continue pushing the frontier of AI capability.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 5 (Similarity: 0.8825459809042107):\n",
      "“We should really focus on making this technology useful for humans and enterprises. That takes time. I believe it’ll be amazing, but [it] doesn’t happen overnight.” The stakes are high for OpenAI to prove that its next major conversational AI, known as a large language model, is significantly better than GPT-4, its most advanced LLM today. OpenAI released GPT-4 a year ago, and Google has released a comparable model in the meantime as it tries to catch up. OpenAI aims to release its next major LLM upgrade by early next year, said one person with knowledge of the process. It could release more incremental improvements to LLMs before then, this person said.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Stargate?\"\n",
    "results_text = query_results(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System message:\n",
      "Answer the question based on the context below, and if the question can't be answered based on the context, say \"I don't know\"\n",
      "\n",
      "\n",
      "User message:\n",
      "Context: While some consumers and professionals have embraced ChatGPT and other conversational AI as well as AI-generated video, turning these recent breakthroughs into technology that produces significant revenue could take longer than practitioners in the field anticipated. Firms including Amazon and Google have quietly tempered expectations for sales, in part because such AI is costly and requires a lot of work to launch inside large enterprises or to power new features in apps used by millions of people. Altman said at an Intel event last month that AI models get “predictably better” when researchers throw more computing power at them. OpenAI has published research on this topic, which it refers to as the “scaling laws” of conversational AI. OpenAI “throwing ever more compute [power to scale up existing AI] risks leading to a ‘trough of disillusionment’” among customers as they realize the limits of the technology, said Ali Ghodsi, CEO of Databricks, which helps companies use AI.\n",
      "\n",
      "###\n",
      "\n",
      "An OpenAI spokesperson did not have a comment for this article. Altman has said privately that Google, one of OpenAI’s biggest rivals, will have more computing capacity than OpenAI in the near term, and publicly he has complained about not having as many AI server chips as he’d like. That’s one reason he has been pitching the idea of a new server chip company that would develop a chip rivaling Nvidia’s graphics processing unit, which today powers OpenAI’s software. Demand for Nvidia GPU servers has skyrocketed, driving up costs for customers such as Microsoft and OpenAI. Besides controlling costs, Microsoft has other potential reasons to support Altman’s alternative chip. The GPU boom has put Nvidia in the position of kingmaker as it decides which customers can have the most chips, and it has aided small cloud providers that compete with Microsoft. Nvidia has also muscled into reselling cloud servers to its own customers.\n",
      "\n",
      "###\n",
      "\n",
      "With more servers available, some OpenAI leaders believe the company can use its existing AI and recent technical breakthroughs such as Q*—a model that can reason about math problems it hasn’t previously been trained to solve—to create the right synthetic (non–human-generated) data for training better models after running out of human-generated data to give them. These models may also be able to figure out the flaws in existing models like GPT-4 and suggest technical improvements—in other words, self-improving AI.\n",
      "\n",
      "###\n",
      "\n",
      "Much of the cost of the next two phases will involve procuring the AI chips. Two data center practitioners who aren’t involved in the project said it’s common for AI server chips to make up around half of the total initial cost of AI-focused data centers other companies are currently building. All up, the proposed efforts could cost in excess of $115 billion, more than three times what Microsoft spent last year on capital expenditures for servers, buildings and other equipment. Microsoft was on pace to spend around $50 billion this year, assuming it continues the pace of capital expenditures it disclosed in the second half of 2023. Microsoft CFO Amy Hood said in January that such spending will increase “materially” in the coming quarters, driven by investments in “cloud and AI infrastructure.” Frank Shaw, a Microsoft spokesperson, did not comment about the supercomputing plans but said in a statement: “We are always planning for the next generation of infrastructure innovations needed to continue pushing the frontier of AI capability.\n",
      "\n",
      "###\n",
      "\n",
      "“We should really focus on making this technology useful for humans and enterprises. That takes time. I believe it’ll be amazing, but [it] doesn’t happen overnight.” The stakes are high for OpenAI to prove that its next major conversational AI, known as a large language model, is significantly better than GPT-4, its most advanced LLM today. OpenAI released GPT-4 a year ago, and Google has released a comparable model in the meantime as it tries to catch up. OpenAI aims to release its next major LLM upgrade by early next year, said one person with knowledge of the process. It could release more incremental improvements to LLMs before then, this person said.\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is Stargate?\n",
      "Answer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know. The context provided does not mention anything about Stargate.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(query, results_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stargate is a popular science fiction franchise that originated with the 1994 film of the same name, directed by Roland Emmerich and starring Kurt Russell and James Spader. The premise centers around a mysterious ancient device called the Stargate, which allows for instant travel to other planets across the galaxy. Following the film’s success, the franchise expanded into several television series, most notably \"Stargate SG-1,\" which debuted in 1997 and ran for ten seasons, as well as spin-offs like \"Stargate Atlantis\" and \"Stargate Universe.\" The shows combine elements of exploration, adventure, and mythology, as a team of military personnel and scientists use the Stargate to explore new worlds, encounter alien civilizations, and defend Earth from threats. The franchise has garnered a dedicated fan base and remains influential in the realm of science fiction.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Given a question, generate a paragraph of text that answers the question.\n",
    "Question: {query}\n",
    "Answer:\n",
    "         \"\"\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=CHUNK_SIZE,\n",
    ")\n",
    "\n",
    "generated_text = response.choices[0].message.content\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1 (Similarity: 0.9120038019165091):\n",
      "With more servers available, some OpenAI leaders believe the company can use its existing AI and recent technical breakthroughs such as Q*—a model that can reason about math problems it hasn’t previously been trained to solve—to create the right synthetic (non–human-generated) data for training better models after running out of human-generated data to give them. These models may also be able to figure out the flaws in existing models like GPT-4 and suggest technical improvements—in other words, self-improving AI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 2 (Similarity: 0.9069401840199753):\n",
      "An OpenAI spokesperson did not have a comment for this article. Altman has said privately that Google, one of OpenAI’s biggest rivals, will have more computing capacity than OpenAI in the near term, and publicly he has complained about not having as many AI server chips as he’d like. That’s one reason he has been pitching the idea of a new server chip company that would develop a chip rivaling Nvidia’s graphics processing unit, which today powers OpenAI’s software. Demand for Nvidia GPU servers has skyrocketed, driving up costs for customers such as Microsoft and OpenAI. Besides controlling costs, Microsoft has other potential reasons to support Altman’s alternative chip. The GPU boom has put Nvidia in the position of kingmaker as it decides which customers can have the most chips, and it has aided small cloud providers that compete with Microsoft. Nvidia has also muscled into reselling cloud servers to its own customers.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 3 (Similarity: 0.8978619924642577):\n",
      "Much of the cost of the next two phases will involve procuring the AI chips. Two data center practitioners who aren’t involved in the project said it’s common for AI server chips to make up around half of the total initial cost of AI-focused data centers other companies are currently building. All up, the proposed efforts could cost in excess of $115 billion, more than three times what Microsoft spent last year on capital expenditures for servers, buildings and other equipment. Microsoft was on pace to spend around $50 billion this year, assuming it continues the pace of capital expenditures it disclosed in the second half of 2023. Microsoft CFO Amy Hood said in January that such spending will increase “materially” in the coming quarters, driven by investments in “cloud and AI infrastructure.” Frank Shaw, a Microsoft spokesperson, did not comment about the supercomputing plans but said in a statement: “We are always planning for the next generation of infrastructure innovations needed to continue pushing the frontier of AI capability.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 4 (Similarity: 0.8949779670028647):\n",
      "“We should really focus on making this technology useful for humans and enterprises. That takes time. I believe it’ll be amazing, but [it] doesn’t happen overnight.” The stakes are high for OpenAI to prove that its next major conversational AI, known as a large language model, is significantly better than GPT-4, its most advanced LLM today. OpenAI released GPT-4 a year ago, and Google has released a comparable model in the meantime as it tries to catch up. OpenAI aims to release its next major LLM upgrade by early next year, said one person with knowledge of the process. It could release more incremental improvements to LLMs before then, this person said.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 5 (Similarity: 0.893548460322185):\n",
      "While some consumers and professionals have embraced ChatGPT and other conversational AI as well as AI-generated video, turning these recent breakthroughs into technology that produces significant revenue could take longer than practitioners in the field anticipated. Firms including Amazon and Google have quietly tempered expectations for sales, in part because such AI is costly and requires a lot of work to launch inside large enterprises or to power new features in apps used by millions of people. Altman said at an Intel event last month that AI models get “predictably better” when researchers throw more computing power at them. OpenAI has published research on this topic, which it refers to as the “scaling laws” of conversational AI. OpenAI “throwing ever more compute [power to scale up existing AI] risks leading to a ‘trough of disillusionment’” among customers as they realize the limits of the technology, said Ali Ghodsi, CEO of Databricks, which helps companies use AI.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generated_results_text = query_results(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System message:\n",
      "Answer the question based on the context below, and if the question can't be answered based on the context, say \"I don't know\"\n",
      "\n",
      "\n",
      "User message:\n",
      "Context: With more servers available, some OpenAI leaders believe the company can use its existing AI and recent technical breakthroughs such as Q*—a model that can reason about math problems it hasn’t previously been trained to solve—to create the right synthetic (non–human-generated) data for training better models after running out of human-generated data to give them. These models may also be able to figure out the flaws in existing models like GPT-4 and suggest technical improvements—in other words, self-improving AI.\n",
      "\n",
      "###\n",
      "\n",
      "An OpenAI spokesperson did not have a comment for this article. Altman has said privately that Google, one of OpenAI’s biggest rivals, will have more computing capacity than OpenAI in the near term, and publicly he has complained about not having as many AI server chips as he’d like. That’s one reason he has been pitching the idea of a new server chip company that would develop a chip rivaling Nvidia’s graphics processing unit, which today powers OpenAI’s software. Demand for Nvidia GPU servers has skyrocketed, driving up costs for customers such as Microsoft and OpenAI. Besides controlling costs, Microsoft has other potential reasons to support Altman’s alternative chip. The GPU boom has put Nvidia in the position of kingmaker as it decides which customers can have the most chips, and it has aided small cloud providers that compete with Microsoft. Nvidia has also muscled into reselling cloud servers to its own customers.\n",
      "\n",
      "###\n",
      "\n",
      "Much of the cost of the next two phases will involve procuring the AI chips. Two data center practitioners who aren’t involved in the project said it’s common for AI server chips to make up around half of the total initial cost of AI-focused data centers other companies are currently building. All up, the proposed efforts could cost in excess of $115 billion, more than three times what Microsoft spent last year on capital expenditures for servers, buildings and other equipment. Microsoft was on pace to spend around $50 billion this year, assuming it continues the pace of capital expenditures it disclosed in the second half of 2023. Microsoft CFO Amy Hood said in January that such spending will increase “materially” in the coming quarters, driven by investments in “cloud and AI infrastructure.” Frank Shaw, a Microsoft spokesperson, did not comment about the supercomputing plans but said in a statement: “We are always planning for the next generation of infrastructure innovations needed to continue pushing the frontier of AI capability.\n",
      "\n",
      "###\n",
      "\n",
      "“We should really focus on making this technology useful for humans and enterprises. That takes time. I believe it’ll be amazing, but [it] doesn’t happen overnight.” The stakes are high for OpenAI to prove that its next major conversational AI, known as a large language model, is significantly better than GPT-4, its most advanced LLM today. OpenAI released GPT-4 a year ago, and Google has released a comparable model in the meantime as it tries to catch up. OpenAI aims to release its next major LLM upgrade by early next year, said one person with knowledge of the process. It could release more incremental improvements to LLMs before then, this person said.\n",
      "\n",
      "###\n",
      "\n",
      "While some consumers and professionals have embraced ChatGPT and other conversational AI as well as AI-generated video, turning these recent breakthroughs into technology that produces significant revenue could take longer than practitioners in the field anticipated. Firms including Amazon and Google have quietly tempered expectations for sales, in part because such AI is costly and requires a lot of work to launch inside large enterprises or to power new features in apps used by millions of people. Altman said at an Intel event last month that AI models get “predictably better” when researchers throw more computing power at them. OpenAI has published research on this topic, which it refers to as the “scaling laws” of conversational AI. OpenAI “throwing ever more compute [power to scale up existing AI] risks leading to a ‘trough of disillusionment’” among customers as they realize the limits of the technology, said Ali Ghodsi, CEO of Databricks, which helps companies use AI.\n",
      "\n",
      "---\n",
      "\n",
      "Question: Stargate is a popular science fiction franchise that originated with the 1994 film of the same name, directed by Roland Emmerich and starring Kurt Russell and James Spader. The premise centers around a mysterious ancient device called the Stargate, which allows for instant travel to other planets across the galaxy. Following the film’s success, the franchise expanded into several television series, most notably \"Stargate SG-1,\" which debuted in 1997 and ran for ten seasons, as well as spin-offs like \"Stargate Atlantis\" and \"Stargate Universe.\" The shows combine elements of exploration, adventure, and mythology, as a team of military personnel and scientists use the Stargate to explore new worlds, encounter alien civilizations, and defend Earth from threats. The franchise has garnered a dedicated fan base and remains influential in the realm of science fiction.\n",
      "Answer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(generated_text, generated_results_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
