{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from ast import literal_eval\n",
    "from embeddings import CHUNK_SIZE\n",
    "from retrieval import query_embeddings\n",
    "\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/20e01e08-12bd-4258-ab45-5cf9244b727f.csv\")\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(literal_eval)\n",
    "texts, embeddings = df[\"text\"].tolist(), df[\"embedding\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_results(query: str, debug=True) -> list[str]:\n",
    "    results = query_embeddings(query, embeddings, 10)\n",
    "    results_text = [texts[i] for i, _ in results]\n",
    "    if debug:\n",
    "        for i, result in enumerate(results_text):\n",
    "            print(f\"Result {i + 1} (Similarity: {results[i][1]}):\")\n",
    "            print(result)\n",
    "            print(\"-\" * 100)\n",
    "    return results_text\n",
    "\n",
    "\n",
    "def ask(query: str, results_text: list[str], debug=True):\n",
    "    context = \"\\n\\n###\\n\\n\".join(results_text)\n",
    "    system_message = \"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\n\"\n",
    "    user_message = f\"Context: {context}\\n\\n---\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    if debug:\n",
    "        print(\"System message:\")\n",
    "        print(system_message)\n",
    "        print(\"User message:\")\n",
    "        print(user_message)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_message,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1 (Similarity: 0.9342499421203767):\n",
      "While some consumers and professionals have embraced ChatGPT and other conversational AI as well as AI-generated video, turning these recent breakthroughs into technology that produces significant revenue could take longer than practitioners in the field anticipated. Firms including Amazon and Google have quietly tempered expectations for sales, in part because such AI is costly and requires a lot of work to launch inside large enterprises or to power new features in apps used by millions of people. Altman said at an Intel event last month that AI models get “predictably better” when researchers throw more computing power at them. OpenAI has published research on this topic, which it refers to as the “scaling laws” of conversational AI. OpenAI “throwing ever more compute [power to scale up existing AI] risks leading to a ‘trough of disillusionment’” among customers as they realize the limits of the technology, said Ali Ghodsi, CEO of Databricks, which helps companies use AI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 2 (Similarity: 0.9138110174776263):\n",
      "An OpenAI spokesperson did not have a comment for this article. Altman has said privately that Google, one of OpenAI’s biggest rivals, will have more computing capacity than OpenAI in the near term, and publicly he has complained about not having as many AI server chips as he’d like. That’s one reason he has been pitching the idea of a new server chip company that would develop a chip rivaling Nvidia’s graphics processing unit, which today powers OpenAI’s software. Demand for Nvidia GPU servers has skyrocketed, driving up costs for customers such as Microsoft and OpenAI. Besides controlling costs, Microsoft has other potential reasons to support Altman’s alternative chip. The GPU boom has put Nvidia in the position of kingmaker as it decides which customers can have the most chips, and it has aided small cloud providers that compete with Microsoft. Nvidia has also muscled into reselling cloud servers to its own customers.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 3 (Similarity: 0.9103444111915934):\n",
      "With more servers available, some OpenAI leaders believe the company can use its existing AI and recent technical breakthroughs such as Q*—a model that can reason about math problems it hasn’t previously been trained to solve—to create the right synthetic (non–human-generated) data for training better models after running out of human-generated data to give them. These models may also be able to figure out the flaws in existing models like GPT-4 and suggest technical improvements—in other words, self-improving AI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 4 (Similarity: 0.9047200469498211):\n",
      "Much of the cost of the next two phases will involve procuring the AI chips. Two data center practitioners who aren’t involved in the project said it’s common for AI server chips to make up around half of the total initial cost of AI-focused data centers other companies are currently building. All up, the proposed efforts could cost in excess of $115 billion, more than three times what Microsoft spent last year on capital expenditures for servers, buildings and other equipment. Microsoft was on pace to spend around $50 billion this year, assuming it continues the pace of capital expenditures it disclosed in the second half of 2023. Microsoft CFO Amy Hood said in January that such spending will increase “materially” in the coming quarters, driven by investments in “cloud and AI infrastructure.” Frank Shaw, a Microsoft spokesperson, did not comment about the supercomputing plans but said in a statement: “We are always planning for the next generation of infrastructure innovations needed to continue pushing the frontier of AI capability.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 5 (Similarity: 0.8825459809042107):\n",
      "“We should really focus on making this technology useful for humans and enterprises. That takes time. I believe it’ll be amazing, but [it] doesn’t happen overnight.” The stakes are high for OpenAI to prove that its next major conversational AI, known as a large language model, is significantly better than GPT-4, its most advanced LLM today. OpenAI released GPT-4 a year ago, and Google has released a comparable model in the meantime as it tries to catch up. OpenAI aims to release its next major LLM upgrade by early next year, said one person with knowledge of the process. It could release more incremental improvements to LLMs before then, this person said.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 6 (Similarity: 0.8736679300895169):\n",
      "(OpenAI instead wants to use more generic Ethernet cables.) Switching away from InfiniBand could make it easier for OpenAI and Microsoft to lessen their reliance on Nvidia down the line. AI computing is more expensive and complex than traditional computing, which is why companies closely guard the details about their AI data centers, including how GPUs are connected and cooled. For his part, Nvidia CEO Jensen Huang has said companies and countries will need to build $1 trillion worth of new data centers in the next four to five years to handle all of the AI computing that’s coming. Microsoft and OpenAI executives have been discussing the data center project since at least last summer.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 7 (Similarity: 0.8670313959502032):\n",
      "Besides CEO Satya Nadella and Chief Technology Officer Kevin Scott, other Microsoft managers who have been involved in the supercomputer talks have included Pradeep Sindhu, who leads strategy for the way Microsoft stitches together AI server chips in its data centers, and Brian Harry, who helps develop AI hardware for the Azure cloud server unit, according to people who have worked with them. OpenAI President Greg Brockman, left, and Microsoft CTO Kevin Scott. Photo via YouTube/Microsoft Developer The partners are still ironing out several key details, which they might not finalize anytime soon. It is unclear where the supercomputer will be physically located and whether it will be built inside one data center or multiple data centers in close proximity. Clusters of GPUs tend to work more efficiently when they are located in the same data center. OpenAI has already pushed the boundaries of what Microsoft can do with data centers.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 8 (Similarity: 0.8282112081835249):\n",
      "After making its initial investment in the startup in 2019, Microsoft built its first GPU supercomputer, containing thousands of Nvidia GPUs, to handle OpenAI’s computing demands, spending $1.2 billion on the system over several years. This year and next year, Microsoft has planned to provide OpenAI with servers housing hundreds of thousands of GPUs in total, said a person with knowledge of its computing needs. The Next Barometer: GPT-5 Microsoft and OpenAI’s grand designs for world-beating data centers depend almost entirely on whether OpenAI can help Microsoft justify the investment in those projects by taking major strides toward superintelligence—AI that can help solve complex problems such as cancer, fusion, global warming or colonizing Mars. Such attainments may be a far-off dream.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 9 (Similarity: 0.7291388577077462):\n",
      "Executives at Microsoft and OpenAI have been drawing up plans for a data center project that would contain a supercomputer with millions of specialized server chips to power OpenAI’s artificial intelligence, according to three people who have been involved in the private conversations about the proposal. The project could cost as much as $100 billion, according to a person who spoke to OpenAI CEO Sam Altman about it and a person who has viewed some of Microsoft’s initial cost estimates. Microsoft would likely be responsible for financing the project, which would be 100 times more costly than some of today’s biggest data centers, demonstrating the enormous investment that may be needed to build computing capacity for AI in the coming years. Executives envisage the proposed U.S.-based supercomputer, which they have referred to as “Stargate,” as the biggest of a series of installations the companies are looking to build over the next six years.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 10 (Similarity: 0.6247877161972781):\n",
      "The Takeaway • Microsoft executives are looking to launch Stargate as soon as 2028 • The supercomputer would require an unprecedented amount of power • OpenAI’s next major AI upgrade is expected to land by early next year While project has not been green-lit and the plans could change, they provide a peek into this decade’s most important tech industry tie-up and how far ahead the two companies are thinking. Microsoft so far has committed more than $13 billion to OpenAI so the startup can use Microsoft data centers to power ChatGPT and the models behind its conversational AI. In exchange, Microsoft gets access to the secret sauce of OpenAI’s technology and the exclusive right to resell that tech to its own cloud customers, such as Morgan Stanley. Microsoft also has baked OpenAI’s software into new AI Copilot features for Office, Teams and Bing.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Stargate?\"\n",
    "results_text = query_results(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System message:\n",
      "Answer the question based on the context below, and if the question can't be answered based on the context, say \"I don't know\"\n",
      "\n",
      "\n",
      "User message:\n",
      "Context: While some consumers and professionals have embraced ChatGPT and other conversational AI as well as AI-generated video, turning these recent breakthroughs into technology that produces significant revenue could take longer than practitioners in the field anticipated. Firms including Amazon and Google have quietly tempered expectations for sales, in part because such AI is costly and requires a lot of work to launch inside large enterprises or to power new features in apps used by millions of people. Altman said at an Intel event last month that AI models get “predictably better” when researchers throw more computing power at them. OpenAI has published research on this topic, which it refers to as the “scaling laws” of conversational AI. OpenAI “throwing ever more compute [power to scale up existing AI] risks leading to a ‘trough of disillusionment’” among customers as they realize the limits of the technology, said Ali Ghodsi, CEO of Databricks, which helps companies use AI.\n",
      "\n",
      "###\n",
      "\n",
      "An OpenAI spokesperson did not have a comment for this article. Altman has said privately that Google, one of OpenAI’s biggest rivals, will have more computing capacity than OpenAI in the near term, and publicly he has complained about not having as many AI server chips as he’d like. That’s one reason he has been pitching the idea of a new server chip company that would develop a chip rivaling Nvidia’s graphics processing unit, which today powers OpenAI’s software. Demand for Nvidia GPU servers has skyrocketed, driving up costs for customers such as Microsoft and OpenAI. Besides controlling costs, Microsoft has other potential reasons to support Altman’s alternative chip. The GPU boom has put Nvidia in the position of kingmaker as it decides which customers can have the most chips, and it has aided small cloud providers that compete with Microsoft. Nvidia has also muscled into reselling cloud servers to its own customers.\n",
      "\n",
      "###\n",
      "\n",
      "With more servers available, some OpenAI leaders believe the company can use its existing AI and recent technical breakthroughs such as Q*—a model that can reason about math problems it hasn’t previously been trained to solve—to create the right synthetic (non–human-generated) data for training better models after running out of human-generated data to give them. These models may also be able to figure out the flaws in existing models like GPT-4 and suggest technical improvements—in other words, self-improving AI.\n",
      "\n",
      "###\n",
      "\n",
      "Much of the cost of the next two phases will involve procuring the AI chips. Two data center practitioners who aren’t involved in the project said it’s common for AI server chips to make up around half of the total initial cost of AI-focused data centers other companies are currently building. All up, the proposed efforts could cost in excess of $115 billion, more than three times what Microsoft spent last year on capital expenditures for servers, buildings and other equipment. Microsoft was on pace to spend around $50 billion this year, assuming it continues the pace of capital expenditures it disclosed in the second half of 2023. Microsoft CFO Amy Hood said in January that such spending will increase “materially” in the coming quarters, driven by investments in “cloud and AI infrastructure.” Frank Shaw, a Microsoft spokesperson, did not comment about the supercomputing plans but said in a statement: “We are always planning for the next generation of infrastructure innovations needed to continue pushing the frontier of AI capability.\n",
      "\n",
      "###\n",
      "\n",
      "“We should really focus on making this technology useful for humans and enterprises. That takes time. I believe it’ll be amazing, but [it] doesn’t happen overnight.” The stakes are high for OpenAI to prove that its next major conversational AI, known as a large language model, is significantly better than GPT-4, its most advanced LLM today. OpenAI released GPT-4 a year ago, and Google has released a comparable model in the meantime as it tries to catch up. OpenAI aims to release its next major LLM upgrade by early next year, said one person with knowledge of the process. It could release more incremental improvements to LLMs before then, this person said.\n",
      "\n",
      "###\n",
      "\n",
      "(OpenAI instead wants to use more generic Ethernet cables.) Switching away from InfiniBand could make it easier for OpenAI and Microsoft to lessen their reliance on Nvidia down the line. AI computing is more expensive and complex than traditional computing, which is why companies closely guard the details about their AI data centers, including how GPUs are connected and cooled. For his part, Nvidia CEO Jensen Huang has said companies and countries will need to build $1 trillion worth of new data centers in the next four to five years to handle all of the AI computing that’s coming. Microsoft and OpenAI executives have been discussing the data center project since at least last summer.\n",
      "\n",
      "###\n",
      "\n",
      "Besides CEO Satya Nadella and Chief Technology Officer Kevin Scott, other Microsoft managers who have been involved in the supercomputer talks have included Pradeep Sindhu, who leads strategy for the way Microsoft stitches together AI server chips in its data centers, and Brian Harry, who helps develop AI hardware for the Azure cloud server unit, according to people who have worked with them. OpenAI President Greg Brockman, left, and Microsoft CTO Kevin Scott. Photo via YouTube/Microsoft Developer The partners are still ironing out several key details, which they might not finalize anytime soon. It is unclear where the supercomputer will be physically located and whether it will be built inside one data center or multiple data centers in close proximity. Clusters of GPUs tend to work more efficiently when they are located in the same data center. OpenAI has already pushed the boundaries of what Microsoft can do with data centers.\n",
      "\n",
      "###\n",
      "\n",
      "After making its initial investment in the startup in 2019, Microsoft built its first GPU supercomputer, containing thousands of Nvidia GPUs, to handle OpenAI’s computing demands, spending $1.2 billion on the system over several years. This year and next year, Microsoft has planned to provide OpenAI with servers housing hundreds of thousands of GPUs in total, said a person with knowledge of its computing needs. The Next Barometer: GPT-5 Microsoft and OpenAI’s grand designs for world-beating data centers depend almost entirely on whether OpenAI can help Microsoft justify the investment in those projects by taking major strides toward superintelligence—AI that can help solve complex problems such as cancer, fusion, global warming or colonizing Mars. Such attainments may be a far-off dream.\n",
      "\n",
      "###\n",
      "\n",
      "Executives at Microsoft and OpenAI have been drawing up plans for a data center project that would contain a supercomputer with millions of specialized server chips to power OpenAI’s artificial intelligence, according to three people who have been involved in the private conversations about the proposal. The project could cost as much as $100 billion, according to a person who spoke to OpenAI CEO Sam Altman about it and a person who has viewed some of Microsoft’s initial cost estimates. Microsoft would likely be responsible for financing the project, which would be 100 times more costly than some of today’s biggest data centers, demonstrating the enormous investment that may be needed to build computing capacity for AI in the coming years. Executives envisage the proposed U.S.-based supercomputer, which they have referred to as “Stargate,” as the biggest of a series of installations the companies are looking to build over the next six years.\n",
      "\n",
      "###\n",
      "\n",
      "The Takeaway • Microsoft executives are looking to launch Stargate as soon as 2028 • The supercomputer would require an unprecedented amount of power • OpenAI’s next major AI upgrade is expected to land by early next year While project has not been green-lit and the plans could change, they provide a peek into this decade’s most important tech industry tie-up and how far ahead the two companies are thinking. Microsoft so far has committed more than $13 billion to OpenAI so the startup can use Microsoft data centers to power ChatGPT and the models behind its conversational AI. In exchange, Microsoft gets access to the secret sauce of OpenAI’s technology and the exclusive right to resell that tech to its own cloud customers, such as Morgan Stanley. Microsoft also has baked OpenAI’s software into new AI Copilot features for Office, Teams and Bing.\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is Stargate?\n",
      "Answer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Stargate is a proposed data center project by Microsoft and OpenAI. It would contain a supercomputer with millions of specialized server chips to power OpenAI's artificial intelligence. The project is estimated to cost as much as $100 billion and is targeted to launch as soon as 2028.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(query, results_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stargate is a science fiction franchise that originated from the 1994 film of the same name, directed by Roland Emmerich. The core concept revolves around a device called the Stargate, which is an ancient, ring-shaped portal that allows for instant travel between different planets across the galaxy. The franchise expanded significantly with the launch of the television series \"Stargate SG-1\" in 1997, which followed a team of military personnel and scientists as they explored various worlds and encountered alien civilizations. This was followed by additional series such as \"Stargate Atlantis\" and \"Stargate Universe,\" along with several television films and an animated series. The franchise explores themes of exploration, teamwork, and the moral complexities of interacting with other cultures, while also delving into mythology and the nature of technology. Stargate has garnered a dedicated fan base and is known for its creative storytelling and rich universe.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Given a question, generate a paragraph of text that answers the question.\n",
    "Question: {query}\n",
    "Answer:\n",
    "         \"\"\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=CHUNK_SIZE,\n",
    ")\n",
    "\n",
    "generated_text = response.choices[0].message.content\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1 (Similarity: 0.914370749553568):\n",
      "An OpenAI spokesperson did not have a comment for this article. Altman has said privately that Google, one of OpenAI’s biggest rivals, will have more computing capacity than OpenAI in the near term, and publicly he has complained about not having as many AI server chips as he’d like. That’s one reason he has been pitching the idea of a new server chip company that would develop a chip rivaling Nvidia’s graphics processing unit, which today powers OpenAI’s software. Demand for Nvidia GPU servers has skyrocketed, driving up costs for customers such as Microsoft and OpenAI. Besides controlling costs, Microsoft has other potential reasons to support Altman’s alternative chip. The GPU boom has put Nvidia in the position of kingmaker as it decides which customers can have the most chips, and it has aided small cloud providers that compete with Microsoft. Nvidia has also muscled into reselling cloud servers to its own customers.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 2 (Similarity: 0.914186907528066):\n",
      "With more servers available, some OpenAI leaders believe the company can use its existing AI and recent technical breakthroughs such as Q*—a model that can reason about math problems it hasn’t previously been trained to solve—to create the right synthetic (non–human-generated) data for training better models after running out of human-generated data to give them. These models may also be able to figure out the flaws in existing models like GPT-4 and suggest technical improvements—in other words, self-improving AI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 3 (Similarity: 0.898762708246427):\n",
      "While some consumers and professionals have embraced ChatGPT and other conversational AI as well as AI-generated video, turning these recent breakthroughs into technology that produces significant revenue could take longer than practitioners in the field anticipated. Firms including Amazon and Google have quietly tempered expectations for sales, in part because such AI is costly and requires a lot of work to launch inside large enterprises or to power new features in apps used by millions of people. Altman said at an Intel event last month that AI models get “predictably better” when researchers throw more computing power at them. OpenAI has published research on this topic, which it refers to as the “scaling laws” of conversational AI. OpenAI “throwing ever more compute [power to scale up existing AI] risks leading to a ‘trough of disillusionment’” among customers as they realize the limits of the technology, said Ali Ghodsi, CEO of Databricks, which helps companies use AI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 4 (Similarity: 0.895441963707871):\n",
      "“We should really focus on making this technology useful for humans and enterprises. That takes time. I believe it’ll be amazing, but [it] doesn’t happen overnight.” The stakes are high for OpenAI to prove that its next major conversational AI, known as a large language model, is significantly better than GPT-4, its most advanced LLM today. OpenAI released GPT-4 a year ago, and Google has released a comparable model in the meantime as it tries to catch up. OpenAI aims to release its next major LLM upgrade by early next year, said one person with knowledge of the process. It could release more incremental improvements to LLMs before then, this person said.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 5 (Similarity: 0.8926003826979275):\n",
      "Much of the cost of the next two phases will involve procuring the AI chips. Two data center practitioners who aren’t involved in the project said it’s common for AI server chips to make up around half of the total initial cost of AI-focused data centers other companies are currently building. All up, the proposed efforts could cost in excess of $115 billion, more than three times what Microsoft spent last year on capital expenditures for servers, buildings and other equipment. Microsoft was on pace to spend around $50 billion this year, assuming it continues the pace of capital expenditures it disclosed in the second half of 2023. Microsoft CFO Amy Hood said in January that such spending will increase “materially” in the coming quarters, driven by investments in “cloud and AI infrastructure.” Frank Shaw, a Microsoft spokesperson, did not comment about the supercomputing plans but said in a statement: “We are always planning for the next generation of infrastructure innovations needed to continue pushing the frontier of AI capability.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 6 (Similarity: 0.8839830346544055):\n",
      "(OpenAI instead wants to use more generic Ethernet cables.) Switching away from InfiniBand could make it easier for OpenAI and Microsoft to lessen their reliance on Nvidia down the line. AI computing is more expensive and complex than traditional computing, which is why companies closely guard the details about their AI data centers, including how GPUs are connected and cooled. For his part, Nvidia CEO Jensen Huang has said companies and countries will need to build $1 trillion worth of new data centers in the next four to five years to handle all of the AI computing that’s coming. Microsoft and OpenAI executives have been discussing the data center project since at least last summer.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 7 (Similarity: 0.868419740618772):\n",
      "Besides CEO Satya Nadella and Chief Technology Officer Kevin Scott, other Microsoft managers who have been involved in the supercomputer talks have included Pradeep Sindhu, who leads strategy for the way Microsoft stitches together AI server chips in its data centers, and Brian Harry, who helps develop AI hardware for the Azure cloud server unit, according to people who have worked with them. OpenAI President Greg Brockman, left, and Microsoft CTO Kevin Scott. Photo via YouTube/Microsoft Developer The partners are still ironing out several key details, which they might not finalize anytime soon. It is unclear where the supercomputer will be physically located and whether it will be built inside one data center or multiple data centers in close proximity. Clusters of GPUs tend to work more efficiently when they are located in the same data center. OpenAI has already pushed the boundaries of what Microsoft can do with data centers.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 8 (Similarity: 0.8372507606595783):\n",
      "After making its initial investment in the startup in 2019, Microsoft built its first GPU supercomputer, containing thousands of Nvidia GPUs, to handle OpenAI’s computing demands, spending $1.2 billion on the system over several years. This year and next year, Microsoft has planned to provide OpenAI with servers housing hundreds of thousands of GPUs in total, said a person with knowledge of its computing needs. The Next Barometer: GPT-5 Microsoft and OpenAI’s grand designs for world-beating data centers depend almost entirely on whether OpenAI can help Microsoft justify the investment in those projects by taking major strides toward superintelligence—AI that can help solve complex problems such as cancer, fusion, global warming or colonizing Mars. Such attainments may be a far-off dream.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 9 (Similarity: 0.7749823108512935):\n",
      "Executives at Microsoft and OpenAI have been drawing up plans for a data center project that would contain a supercomputer with millions of specialized server chips to power OpenAI’s artificial intelligence, according to three people who have been involved in the private conversations about the proposal. The project could cost as much as $100 billion, according to a person who spoke to OpenAI CEO Sam Altman about it and a person who has viewed some of Microsoft’s initial cost estimates. Microsoft would likely be responsible for financing the project, which would be 100 times more costly than some of today’s biggest data centers, demonstrating the enormous investment that may be needed to build computing capacity for AI in the coming years. Executives envisage the proposed U.S.-based supercomputer, which they have referred to as “Stargate,” as the biggest of a series of installations the companies are looking to build over the next six years.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 10 (Similarity: 0.6781118373477875):\n",
      "(The codename originated with OpenAI but isn’t the official project codename that Microsoft is using, said one person who has been involved.) The phase prior to Stargate would cost far less. Microsoft is working on a smaller, phase 4 supercomputer for OpenAI that it aims to launch around 2026, according to two of the people. Executives have planned to build it in Mt. Pleasant, Wisc., where the Wisconsin Economic Development Corporation recently said Microsoft broke ground on a $1 billion data center expansion. The supercomputer and data center could eventually cost as much as $10 billion to complete, one of these people said. That’s many times more than the cost of existing data centers. Microsoft also has discussed using Nvidia-made AI chips for that project, said a different person who has been involved in the conversations. Today, Microsoft and OpenAI are in the middle of phase 3 of the five-phase plan.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generated_results_text = query_results(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System message:\n",
      "Answer the question based on the context below, and if the question can't be answered based on the context, say \"I don't know\"\n",
      "\n",
      "\n",
      "User message:\n",
      "Context: An OpenAI spokesperson did not have a comment for this article. Altman has said privately that Google, one of OpenAI’s biggest rivals, will have more computing capacity than OpenAI in the near term, and publicly he has complained about not having as many AI server chips as he’d like. That’s one reason he has been pitching the idea of a new server chip company that would develop a chip rivaling Nvidia’s graphics processing unit, which today powers OpenAI’s software. Demand for Nvidia GPU servers has skyrocketed, driving up costs for customers such as Microsoft and OpenAI. Besides controlling costs, Microsoft has other potential reasons to support Altman’s alternative chip. The GPU boom has put Nvidia in the position of kingmaker as it decides which customers can have the most chips, and it has aided small cloud providers that compete with Microsoft. Nvidia has also muscled into reselling cloud servers to its own customers.\n",
      "\n",
      "###\n",
      "\n",
      "With more servers available, some OpenAI leaders believe the company can use its existing AI and recent technical breakthroughs such as Q*—a model that can reason about math problems it hasn’t previously been trained to solve—to create the right synthetic (non–human-generated) data for training better models after running out of human-generated data to give them. These models may also be able to figure out the flaws in existing models like GPT-4 and suggest technical improvements—in other words, self-improving AI.\n",
      "\n",
      "###\n",
      "\n",
      "While some consumers and professionals have embraced ChatGPT and other conversational AI as well as AI-generated video, turning these recent breakthroughs into technology that produces significant revenue could take longer than practitioners in the field anticipated. Firms including Amazon and Google have quietly tempered expectations for sales, in part because such AI is costly and requires a lot of work to launch inside large enterprises or to power new features in apps used by millions of people. Altman said at an Intel event last month that AI models get “predictably better” when researchers throw more computing power at them. OpenAI has published research on this topic, which it refers to as the “scaling laws” of conversational AI. OpenAI “throwing ever more compute [power to scale up existing AI] risks leading to a ‘trough of disillusionment’” among customers as they realize the limits of the technology, said Ali Ghodsi, CEO of Databricks, which helps companies use AI.\n",
      "\n",
      "###\n",
      "\n",
      "“We should really focus on making this technology useful for humans and enterprises. That takes time. I believe it’ll be amazing, but [it] doesn’t happen overnight.” The stakes are high for OpenAI to prove that its next major conversational AI, known as a large language model, is significantly better than GPT-4, its most advanced LLM today. OpenAI released GPT-4 a year ago, and Google has released a comparable model in the meantime as it tries to catch up. OpenAI aims to release its next major LLM upgrade by early next year, said one person with knowledge of the process. It could release more incremental improvements to LLMs before then, this person said.\n",
      "\n",
      "###\n",
      "\n",
      "Much of the cost of the next two phases will involve procuring the AI chips. Two data center practitioners who aren’t involved in the project said it’s common for AI server chips to make up around half of the total initial cost of AI-focused data centers other companies are currently building. All up, the proposed efforts could cost in excess of $115 billion, more than three times what Microsoft spent last year on capital expenditures for servers, buildings and other equipment. Microsoft was on pace to spend around $50 billion this year, assuming it continues the pace of capital expenditures it disclosed in the second half of 2023. Microsoft CFO Amy Hood said in January that such spending will increase “materially” in the coming quarters, driven by investments in “cloud and AI infrastructure.” Frank Shaw, a Microsoft spokesperson, did not comment about the supercomputing plans but said in a statement: “We are always planning for the next generation of infrastructure innovations needed to continue pushing the frontier of AI capability.\n",
      "\n",
      "###\n",
      "\n",
      "(OpenAI instead wants to use more generic Ethernet cables.) Switching away from InfiniBand could make it easier for OpenAI and Microsoft to lessen their reliance on Nvidia down the line. AI computing is more expensive and complex than traditional computing, which is why companies closely guard the details about their AI data centers, including how GPUs are connected and cooled. For his part, Nvidia CEO Jensen Huang has said companies and countries will need to build $1 trillion worth of new data centers in the next four to five years to handle all of the AI computing that’s coming. Microsoft and OpenAI executives have been discussing the data center project since at least last summer.\n",
      "\n",
      "###\n",
      "\n",
      "Besides CEO Satya Nadella and Chief Technology Officer Kevin Scott, other Microsoft managers who have been involved in the supercomputer talks have included Pradeep Sindhu, who leads strategy for the way Microsoft stitches together AI server chips in its data centers, and Brian Harry, who helps develop AI hardware for the Azure cloud server unit, according to people who have worked with them. OpenAI President Greg Brockman, left, and Microsoft CTO Kevin Scott. Photo via YouTube/Microsoft Developer The partners are still ironing out several key details, which they might not finalize anytime soon. It is unclear where the supercomputer will be physically located and whether it will be built inside one data center or multiple data centers in close proximity. Clusters of GPUs tend to work more efficiently when they are located in the same data center. OpenAI has already pushed the boundaries of what Microsoft can do with data centers.\n",
      "\n",
      "###\n",
      "\n",
      "After making its initial investment in the startup in 2019, Microsoft built its first GPU supercomputer, containing thousands of Nvidia GPUs, to handle OpenAI’s computing demands, spending $1.2 billion on the system over several years. This year and next year, Microsoft has planned to provide OpenAI with servers housing hundreds of thousands of GPUs in total, said a person with knowledge of its computing needs. The Next Barometer: GPT-5 Microsoft and OpenAI’s grand designs for world-beating data centers depend almost entirely on whether OpenAI can help Microsoft justify the investment in those projects by taking major strides toward superintelligence—AI that can help solve complex problems such as cancer, fusion, global warming or colonizing Mars. Such attainments may be a far-off dream.\n",
      "\n",
      "###\n",
      "\n",
      "Executives at Microsoft and OpenAI have been drawing up plans for a data center project that would contain a supercomputer with millions of specialized server chips to power OpenAI’s artificial intelligence, according to three people who have been involved in the private conversations about the proposal. The project could cost as much as $100 billion, according to a person who spoke to OpenAI CEO Sam Altman about it and a person who has viewed some of Microsoft’s initial cost estimates. Microsoft would likely be responsible for financing the project, which would be 100 times more costly than some of today’s biggest data centers, demonstrating the enormous investment that may be needed to build computing capacity for AI in the coming years. Executives envisage the proposed U.S.-based supercomputer, which they have referred to as “Stargate,” as the biggest of a series of installations the companies are looking to build over the next six years.\n",
      "\n",
      "###\n",
      "\n",
      "(The codename originated with OpenAI but isn’t the official project codename that Microsoft is using, said one person who has been involved.) The phase prior to Stargate would cost far less. Microsoft is working on a smaller, phase 4 supercomputer for OpenAI that it aims to launch around 2026, according to two of the people. Executives have planned to build it in Mt. Pleasant, Wisc., where the Wisconsin Economic Development Corporation recently said Microsoft broke ground on a $1 billion data center expansion. The supercomputer and data center could eventually cost as much as $10 billion to complete, one of these people said. That’s many times more than the cost of existing data centers. Microsoft also has discussed using Nvidia-made AI chips for that project, said a different person who has been involved in the conversations. Today, Microsoft and OpenAI are in the middle of phase 3 of the five-phase plan.\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is Stargate?\n",
      "Answer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Stargate is the proposed U.S.-based supercomputer that Microsoft and OpenAI are planning to build. It is envisioned as the largest of a series of installations aimed at significantly increasing computing capacity for AI. The project could cost as much as $100 billion, according to initial cost estimates.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(query, generated_results_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
