{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gge/gg2001/prompt-playground/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo = dspy.OpenAI(model=\"gpt-3.5-turbo-1106\", max_tokens=250, model_type=\"chat\")\n",
    "dspy.settings.configure(lm=turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4T = dspy.OpenAI(model=\"gpt-4-1106-preview\", max_tokens=350, model_type=\"chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scone(dirname):\n",
    "    dfs = []\n",
    "    for filename in glob.glob(dirname + \"/*.csv\"):\n",
    "        df = pd.read_csv(filename, index_col=0)\n",
    "        df[\"category\"] = os.path.basename(filename).replace(\".csv\", \"\")\n",
    "        dfs.append(df)\n",
    "    data_df = pd.concat(dfs)\n",
    "\n",
    "    def as_example(row):\n",
    "        # The 'one_scoped' file is from an earlier dataset, MoNLI, and\n",
    "        # so is formatted a bit differently:\n",
    "        suffix = \"\" if row[\"category\"] == \"one_scoped\" else \"_edited\"\n",
    "        # Reformat the hypothesis to be an embedded clause in a question:\n",
    "        hkey = \"sentence2\" + suffix\n",
    "        question = row[hkey][0].lower() + row[hkey][1:].strip(\".\")\n",
    "        question = f\"Can we logically conclude for sure that {question}?\"\n",
    "        # Binary task formulation:\n",
    "        label = \"Yes\" if row[\"gold_label\" + suffix] == \"entailment\" else \"No\"\n",
    "        return dspy.Example(\n",
    "            {\n",
    "                \"context\": row[\"sentence1\" + suffix],\n",
    "                \"question\": question,\n",
    "                \"answer\": label,\n",
    "                \"category\": row[\"category\"],\n",
    "            }\n",
    "        ).with_inputs(\"context\", \"question\")\n",
    "\n",
    "    return list(data_df.apply(as_example, axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train = load_scone(\"ScoNe/scone_nli/train\")\n",
    "\n",
    "random.seed(1)\n",
    "random.shuffle(all_train)\n",
    "\n",
    "# 200 random train, 50 random dev:\n",
    "train, dev = all_train[:200], all_train[200:250]\n",
    "\n",
    "len(train), len(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: The people are not happy when they play instruments.\n",
      "Question: Can we logically conclude for sure that the people are not happy when they play accordions?\n",
      "Answer: No\n",
      "Category: one_not_scoped\n"
     ]
    }
   ],
   "source": [
    "example = dev[0]\n",
    "print(\"Context:\", example.context)\n",
    "print(\"Question:\", example.question)\n",
    "print(\"Answer:\", example.answer)\n",
    "print(\"Category:\", example.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "\n",
    "test = load_scone(dirname=\"ScoNe/scone_nli/test\")\n",
    "\n",
    "# We're developing a system for the full ScoNe benchmark, but we'll\n",
    "# evaluate only on one of the hardest and most informative ScoNe\n",
    "# categories for now -- examples with a single negation that plays\n",
    "# a crucial role in the reasoning:\n",
    "test = [ex for ex in test if ex.category == \"one_scoped\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     100\n",
       "Yes    100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([ex.answer for ex in test]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scone_accuracy = dspy.evaluate.metrics.answer_exact_match\n",
    "evaluator = Evaluate(devset=test, num_threads=1, display_progress=True, display_table=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoNeSignature(dspy.Signature):\n",
    "    (\n",
    "        \"\"\"You are given some context (a premise) and a question (a hypothesis). \"\"\"\n",
    "        \"\"\"You must indicate with Yes/No answer whether we can logically \"\"\"\n",
    "        \"\"\"conclude the hypothesis from the premise.\"\"\"\n",
    "    )\n",
    "\n",
    "    context = dspy.InputField()\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"Yes or No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoNeCoT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.ChainOfThought(ScoNeSignature)\n",
    "\n",
    "    def forward(self, context, question):\n",
    "        return self.generate_answer(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_zeroshot = ScoNeCoT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 100 / 200  (50.0): 100%|██████████| 200/200 [03:21<00:00,  1.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator(cot_zeroshot, metric=scone_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 8 traces per predictor.\n",
      "Will attempt to bootstrap 10 candidate sets.\n"
     ]
    }
   ],
   "source": [
    "bootstrap_optimizer = BootstrapFewShotWithRandomSearch(\n",
    "    max_bootstrapped_demos=8,\n",
    "    max_labeled_demos=8,\n",
    "    num_candidate_programs=10,\n",
    "    num_threads=8,\n",
    "    metric=scone_accuracy,\n",
    "    teacher_settings=dict(lm=gpt4T),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 22 / 50  (44.0): 100%|██████████| 50/50 [00:06<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 44.0 for set: [0]\n",
      "New best sscore: 44.0 for seed -3\n",
      "Scores so far: [44.0]\n",
      "Best score: 44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 28 / 50  (56.0): 100%|██████████| 50/50 [00:06<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 56.0 for set: [8]\n",
      "New best sscore: 56.0 for seed -2\n",
      "Scores so far: [44.0, 56.0]\n",
      "Best score: 56.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [00:33<09:41,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 12 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 27 / 50  (54.0): 100%|██████████| 50/50 [00:08<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 54.0 for set: [8]\n",
      "Scores so far: [44.0, 56.0, 54.0]\n",
      "Best score: 56.0\n",
      "Average of max per entry across top 1 scores: 0.56\n",
      "Average of max per entry across top 2 scores: 0.74\n",
      "Average of max per entry across top 3 scores: 0.76\n",
      "Average of max per entry across top 5 scores: 0.76\n",
      "Average of max per entry across top 8 scores: 0.76\n",
      "Average of max per entry across top 9999 scores: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/200 [00:47<16:52,  5.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 7 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 35 / 50  (70.0): 100%|██████████| 50/50 [00:07<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 70.0 for set: [8]\n",
      "New best sscore: 70.0 for seed 0\n",
      "Scores so far: [44.0, 56.0, 54.0, 70.0]\n",
      "Best score: 70.0\n",
      "Average of max per entry across top 1 scores: 0.7\n",
      "Average of max per entry across top 2 scores: 0.86\n",
      "Average of max per entry across top 3 scores: 0.92\n",
      "Average of max per entry across top 5 scores: 0.94\n",
      "Average of max per entry across top 8 scores: 0.94\n",
      "Average of max per entry across top 9999 scores: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [00:09<10:32,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 28 / 50  (56.0): 100%|██████████| 50/50 [00:07<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 56.0 for set: [8]\n",
      "Scores so far: [44.0, 56.0, 54.0, 70.0, 56.0]\n",
      "Best score: 70.0\n",
      "Average of max per entry across top 1 scores: 0.7\n",
      "Average of max per entry across top 2 scores: 0.86\n",
      "Average of max per entry across top 3 scores: 0.88\n",
      "Average of max per entry across top 5 scores: 0.96\n",
      "Average of max per entry across top 8 scores: 0.96\n",
      "Average of max per entry across top 9999 scores: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:02<08:45,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 37 / 50  (74.0): 100%|██████████| 50/50 [00:08<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 74.0 for set: [8]\n",
      "New best sscore: 74.0 for seed 2\n",
      "Scores so far: [44.0, 56.0, 54.0, 70.0, 56.0, 74.0]\n",
      "Best score: 74.0\n",
      "Average of max per entry across top 1 scores: 0.74\n",
      "Average of max per entry across top 2 scores: 0.88\n",
      "Average of max per entry across top 3 scores: 0.94\n",
      "Average of max per entry across top 5 scores: 0.96\n",
      "Average of max per entry across top 8 scores: 0.98\n",
      "Average of max per entry across top 9999 scores: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [00:11<09:39,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 22 / 50  (44.0): 100%|██████████| 50/50 [00:07<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 44.0 for set: [8]\n",
      "Scores so far: [44.0, 56.0, 54.0, 70.0, 56.0, 74.0, 44.0]\n",
      "Best score: 74.0\n",
      "Average of max per entry across top 1 scores: 0.74\n",
      "Average of max per entry across top 2 scores: 0.88\n",
      "Average of max per entry across top 3 scores: 0.94\n",
      "Average of max per entry across top 5 scores: 0.96\n",
      "Average of max per entry across top 8 scores: 0.98\n",
      "Average of max per entry across top 9999 scores: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [00:12<09:53,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 30 / 50  (60.0): 100%|██████████| 50/50 [00:06<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 60.0 for set: [8]\n",
      "Scores so far: [44.0, 56.0, 54.0, 70.0, 56.0, 74.0, 44.0, 60.0]\n",
      "Best score: 74.0\n",
      "Average of max per entry across top 1 scores: 0.74\n",
      "Average of max per entry across top 2 scores: 0.88\n",
      "Average of max per entry across top 3 scores: 0.9\n",
      "Average of max per entry across top 5 scores: 0.96\n",
      "Average of max per entry across top 8 scores: 0.98\n",
      "Average of max per entry across top 9999 scores: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 5/200 [00:15<09:47,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 5 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 32 / 50  (64.0): 100%|██████████| 50/50 [00:07<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 64.0 for set: [8]\n",
      "Scores so far: [44.0, 56.0, 54.0, 70.0, 56.0, 74.0, 44.0, 60.0, 64.0]\n",
      "Best score: 74.0\n",
      "Average of max per entry across top 1 scores: 0.74\n",
      "Average of max per entry across top 2 scores: 0.88\n",
      "Average of max per entry across top 3 scores: 0.92\n",
      "Average of max per entry across top 5 scores: 1.0\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:04<07:16,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 28 / 50  (56.0): 100%|██████████| 50/50 [00:07<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 56.0 for set: [8]\n",
      "Scores so far: [44.0, 56.0, 54.0, 70.0, 56.0, 74.0, 44.0, 60.0, 64.0, 56.0]\n",
      "Best score: 74.0\n",
      "Average of max per entry across top 1 scores: 0.74\n",
      "Average of max per entry across top 2 scores: 0.88\n",
      "Average of max per entry across top 3 scores: 0.92\n",
      "Average of max per entry across top 5 scores: 1.0\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 7/200 [00:22<10:16,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 6 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 30 / 50  (60.0): 100%|██████████| 50/50 [00:08<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 60.0 for set: [8]\n",
      "Scores so far: [44.0, 56.0, 54.0, 70.0, 56.0, 74.0, 44.0, 60.0, 64.0, 56.0, 60.0]\n",
      "Best score: 74.0\n",
      "Average of max per entry across top 1 scores: 0.74\n",
      "Average of max per entry across top 2 scores: 0.88\n",
      "Average of max per entry across top 3 scores: 0.92\n",
      "Average of max per entry across top 5 scores: 1.0\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 5/200 [00:14<09:36,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 50  (52.0): 100%|██████████| 50/50 [00:07<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 52.0 for set: [8]\n",
      "Scores so far: [44.0, 56.0, 54.0, 70.0, 56.0, 74.0, 44.0, 60.0, 64.0, 56.0, 60.0, 52.0]\n",
      "Best score: 74.0\n",
      "Average of max per entry across top 1 scores: 0.74\n",
      "Average of max per entry across top 2 scores: 0.88\n",
      "Average of max per entry across top 3 scores: 0.92\n",
      "Average of max per entry across top 5 scores: 1.0\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [00:22<09:02,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 29 / 50  (58.0): 100%|██████████| 50/50 [00:07<00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 58.0 for set: [8]\n",
      "Scores so far: [44.0, 56.0, 54.0, 70.0, 56.0, 74.0, 44.0, 60.0, 64.0, 56.0, 60.0, 52.0, 58.0]\n",
      "Best score: 74.0\n",
      "Average of max per entry across top 1 scores: 0.74\n",
      "Average of max per entry across top 2 scores: 0.88\n",
      "Average of max per entry across top 3 scores: 0.92\n",
      "Average of max per entry across top 5 scores: 1.0\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n",
      "13 candidate programs found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cot_fewshot = bootstrap_optimizer.compile(cot_zeroshot, trainset=train, valset=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 139 / 200  (69.5): 100%|██████████| 200/200 [03:31<00:00,  1.06s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator(cot_fewshot, metric=scone_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "You are given some context (a premise) and a question (a hypothesis). You must indicate with Yes/No answer whether we can logically conclude the hypothesis from the premise.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: ${context}\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: Yes or No\n",
      "\n",
      "---\n",
      "\n",
      "Context: It is a lie that a boy is not concentrating on a motor\n",
      "\n",
      "Question: Can we logically conclude for sure that it is a lie that a boy is not concentrating on a machine?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the answer. We know that the statement \"a boy is not concentrating on a motor\" is a lie. This means the opposite is true, so the boy is concentrating on a motor. A motor is a type of machine. Therefore, if the boy is concentrating on a motor, he is concentrating on a machine.\n",
      "\n",
      "Answer: Yes\n",
      "\n",
      "---\n",
      "\n",
      "Context: A man is holding something in his hands.\n",
      "Question: Can we logically conclude for sure that a man is holding rats in his hands?\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "\n",
      "Context: It is a lie that the man is not listening to bluegrass.\n",
      "Question: Can we logically conclude for sure that it is a lie that the man is not listening to music?\n",
      "Answer: Yes\n",
      "\n",
      "---\n",
      "\n",
      "Context: It is a lie that the people were outside trying to keep their voices down so as not to waken the wife indoors.\n",
      "Question: Can we logically conclude for sure that it is a lie that the people were outside trying to keep their voices down so as not to waken the woman indoors?\n",
      "Answer: Yes\n",
      "\n",
      "---\n",
      "\n",
      "Context: There is a single person walking in the city that does not sleep and does not rest.\n",
      "Question: Can we logically conclude for sure that there is a single greek walking in the city that does not sleep and does not rest?\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "\n",
      "Context: To believe that a man walks confidently not dropping produce is to believe a falsity.\n",
      "Question: Can we logically conclude for sure that to believe that a man walks confidently not dropping pears is to believe a falsity?\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "\n",
      "Context: The people are playing instruments.\n",
      "Question: Can we logically conclude for sure that the people are playing trombones?\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "\n",
      "Context: There is a man not wearing a fedora, and he is not staring at people on a subway.\n",
      "Question: Can we logically conclude for sure that there is a man not wearing a hat, and he is not staring at people on a subway?\n",
      "Answer: No\n",
      "\n",
      "---\n",
      "\n",
      "Context: The man is not steering a sedan\n",
      "\n",
      "Question: Can we logically conclude for sure that the man is not steering a car?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We know that a sedan is a type of car. So if the man is not steering a sedan, it does not necessarily mean he is not steering a car. He could be steering a different type of car, such as a truck or a van.\n",
      "\n",
      "Answer: No\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nYou are given some context (a premise) and a question (a hypothesis). You must indicate with Yes/No answer whether we can logically conclude the hypothesis from the premise.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: ${context}\\n\\nQuestion: ${question}\\n\\nReasoning: Let\\'s think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: Yes or No\\n\\n---\\n\\nContext: It is a lie that a boy is not concentrating on a motor\\n\\nQuestion: Can we logically conclude for sure that it is a lie that a boy is not concentrating on a machine?\\n\\nReasoning: Let\\'s think step by step in order to produce the answer. We know that the statement \"a boy is not concentrating on a motor\" is a lie. This means the opposite is true, so the boy is concentrating on a motor. A motor is a type of machine. Therefore, if the boy is concentrating on a motor, he is concentrating on a machine.\\n\\nAnswer: Yes\\n\\n---\\n\\nContext: A man is holding something in his hands.\\nQuestion: Can we logically conclude for sure that a man is holding rats in his hands?\\nAnswer: No\\n\\n---\\n\\nContext: It is a lie that the man is not listening to bluegrass.\\nQuestion: Can we logically conclude for sure that it is a lie that the man is not listening to music?\\nAnswer: Yes\\n\\n---\\n\\nContext: It is a lie that the people were outside trying to keep their voices down so as not to waken the wife indoors.\\nQuestion: Can we logically conclude for sure that it is a lie that the people were outside trying to keep their voices down so as not to waken the woman indoors?\\nAnswer: Yes\\n\\n---\\n\\nContext: There is a single person walking in the city that does not sleep and does not rest.\\nQuestion: Can we logically conclude for sure that there is a single greek walking in the city that does not sleep and does not rest?\\nAnswer: No\\n\\n---\\n\\nContext: To believe that a man walks confidently not dropping produce is to believe a falsity.\\nQuestion: Can we logically conclude for sure that to believe that a man walks confidently not dropping pears is to believe a falsity?\\nAnswer: No\\n\\n---\\n\\nContext: The people are playing instruments.\\nQuestion: Can we logically conclude for sure that the people are playing trombones?\\nAnswer: No\\n\\n---\\n\\nContext: There is a man not wearing a fedora, and he is not staring at people on a subway.\\nQuestion: Can we logically conclude for sure that there is a man not wearing a hat, and he is not staring at people on a subway?\\nAnswer: No\\n\\n---\\n\\nContext: The man is not steering a sedan\\n\\nQuestion: Can we logically conclude for sure that the man is not steering a car?\\n\\nReasoning: Let\\'s think step by step in order to\\x1b[32m produce the answer. We know that a sedan is a type of car. So if the man is not steering a sedan, it does not necessarily mean he is not steering a car. He could be steering a different type of car, such as a truck or a van.\\n\\nAnswer: No\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turbo.inspect_history(n=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
